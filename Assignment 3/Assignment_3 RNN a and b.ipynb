{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b408ad95",
   "metadata": {
    "id": "b408ad95"
   },
   "source": [
    "## Installed Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31912917",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31912917",
    "outputId": "e0d133d3-7acf-47bb-c09c-31d1f3472af2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting pyahocorasick\n",
      "  Downloading pyahocorasick-2.0.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (104 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.5/104.5 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting anyascii\n",
      "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 KB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.1 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting gensim==4.2.0\n",
      "  Downloading gensim-4.2.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.8/dist-packages (from gensim==4.2.0) (1.22.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim==4.2.0) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim==4.2.0) (6.3.0)\n",
      "Installing collected packages: gensim\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 3.6.0\n",
      "    Uninstalling gensim-3.6.0:\n",
      "      Successfully uninstalled gensim-3.6.0\n",
      "Successfully installed gensim-4.2.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.22.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.1+cu116)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.8/dist-packages (0.13.1+cu116)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.25.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.22.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install contractions\n",
    "!{sys.executable} -m pip install gensim==4.2.0\n",
    "!pip install scikit-learn\n",
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9cac7ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9cac7ca",
    "outputId": "c3931203-30bd-4062-dc5b-34672b51909a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "## Importing and installing libraries\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import warnings \n",
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from torch import nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss, Softmax, Linear\n",
    "from torch.optim import SGD, Adam\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import utils\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from statistics import mean\n",
    "from os import path\n",
    "import os.path\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50330ee0",
   "metadata": {
    "id": "50330ee0"
   },
   "source": [
    "## 1. Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "XthACUqG0PBl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XthACUqG0PBl",
    "outputId": "8a6ea11d-dd54-4483-a69a-71bcaa2c174f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "# %cd /content/drive/My Drive/Colab Notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "R3ctnM_ToYJv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3ctnM_ToYJv",
    "outputId": "4fb5f7cf-29d7-44c4-b5d8-8383ceac9e9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/My Drive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "791a4b96",
   "metadata": {
    "id": "791a4b96"
   },
   "outputs": [],
   "source": [
    "#fields required in the balanced dataframe from the original dataset\n",
    "input_column=[\"review_body\",\"star_rating\"]\n",
    "\n",
    "#reading the original dataset to filter the columns that are required\n",
    "input_df =pd.read_csv('./amazon_reviews_us_Beauty_v1_00.tsv',usecols=input_column,sep='\\t',error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3317dfe3",
   "metadata": {
    "id": "3317dfe3"
   },
   "outputs": [],
   "source": [
    "#Creating 3 different classes to get 20000 data from each class to avoid computational burden\n",
    "\n",
    "class_one_df =(input_df[(input_df['star_rating'] == 1) | (input_df['star_rating'] == 2) ]).sample(n=20000)\n",
    "class_one_df['class']=1\n",
    "\n",
    "class_two_df =(input_df[(input_df['star_rating'] == 3)]).sample(n=20000)\n",
    "class_two_df['class']=2\n",
    "\n",
    "class_three_df =(input_df[(input_df['star_rating'] == 4) | (input_df['star_rating'] == 5) ]).sample(n=20000)\n",
    "class_three_df['class']=3\n",
    "\n",
    "#Combining all the data received from each class into a single balanced dataframe\n",
    "\n",
    "amazon_balanced_df = pd.concat([class_one_df, class_two_df, class_three_df])\n",
    "\n",
    "#Resetting the index as we have retrieved different data according to the classes created.\n",
    "#Therefore, we will have irregular or unsorted index keys. \n",
    "#We will reset the index to the new and incremental values from 0\n",
    "\n",
    "amazon_balanced_df = amazon_balanced_df.reset_index(drop=True)\n",
    "\n",
    "# Created a new dataframe consisting of the two columns (star_rating and review_body) \n",
    "#along with class one assigned to them on the basis of star_rating. We are also resetting the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c26f1c1",
   "metadata": {
    "id": "2c26f1c1"
   },
   "source": [
    "### Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3d5ab5",
   "metadata": {
    "id": "2f3d5ab5"
   },
   "source": [
    "### Handling null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b712481",
   "metadata": {
    "id": "3b712481"
   },
   "outputs": [],
   "source": [
    "#We are changing all null values to an empty string\n",
    "\n",
    "amazon_balanced_df = amazon_balanced_df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "816443d1",
   "metadata": {
    "id": "816443d1"
   },
   "outputs": [],
   "source": [
    "#Uncleaned data copy\n",
    "amazon_df=amazon_balanced_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de263c8c",
   "metadata": {
    "id": "de263c8c"
   },
   "source": [
    "### Convert all reviews into lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e7707d5",
   "metadata": {
    "id": "6e7707d5"
   },
   "outputs": [],
   "source": [
    "# Converting all review body into lowercase\n",
    "\n",
    "amazon_balanced_df['review_body'] = amazon_balanced_df['review_body'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148e2913",
   "metadata": {
    "id": "148e2913"
   },
   "source": [
    "### Remove the HTML from the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32ca361f",
   "metadata": {
    "id": "32ca361f"
   },
   "outputs": [],
   "source": [
    "# Removing all the html tags from each review body \n",
    "\n",
    "amazon_balanced_df['review_body']=amazon_balanced_df['review_body'].apply(lambda x : re.sub('<.*?>','',str(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e00124",
   "metadata": {
    "id": "c7e00124"
   },
   "source": [
    "### Remove the URLs from the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6904642",
   "metadata": {
    "id": "e6904642"
   },
   "outputs": [],
   "source": [
    "# Removing all the URLs from each review body \n",
    "\n",
    "amazon_balanced_df['review_body'] = amazon_balanced_df['review_body'].apply(lambda y: re.split('https:\\/\\/.*', str(y))[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec91edcf",
   "metadata": {
    "id": "ec91edcf"
   },
   "source": [
    "### Remove non-alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43757d50",
   "metadata": {
    "id": "43757d50"
   },
   "outputs": [],
   "source": [
    "# Removing all the non alphabetic chaarcters(symbols, numbers) from each review body \n",
    "\n",
    "amazon_balanced_df['review_body'] = amazon_balanced_df['review_body'].apply(lambda z: \" \".join([re.sub('[^A-Za-z]+','', z) for z in nltk.word_tokenize(z)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5e594d",
   "metadata": {
    "id": "3e5e594d"
   },
   "source": [
    "### Remove extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d078c690",
   "metadata": {
    "id": "d078c690"
   },
   "outputs": [],
   "source": [
    "# Will remove leading and trailing spaces\n",
    "amazon_balanced_df['review_body'] = amazon_balanced_df['review_body'].str.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7769377",
   "metadata": {
    "id": "d7769377"
   },
   "source": [
    "### Perform contractions on the review_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f866c40",
   "metadata": {
    "id": "0f866c40"
   },
   "outputs": [],
   "source": [
    "## This will elongate the short form used in sentences like (I'll ---> I will)\n",
    "\n",
    "amazon_balanced_df['without_contraction'] = amazon_balanced_df['review_body'].apply(lambda a: [contractions.fix(word) for word in a.split()])\n",
    "amazon_balanced_df['review_body'] = [' '.join(map(str, x)) for x in amazon_balanced_df['without_contraction']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d8987",
   "metadata": {
    "id": "eb8d8987"
   },
   "source": [
    "## Remove Punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7a428a2",
   "metadata": {
    "id": "f7a428a2"
   },
   "outputs": [],
   "source": [
    "amazon_balanced_df['review_body'] = amazon_balanced_df['review_body'].str.replace(r'[^\\w\\s]+', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6866fdc8",
   "metadata": {
    "id": "6866fdc8"
   },
   "source": [
    "## 2. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4d2c8d",
   "metadata": {
    "id": "0f4d2c8d"
   },
   "source": [
    "### (a) Downloading pretrained word2vec-google-news-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "hwM_SlhDijgG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hwM_SlhDijgG",
    "outputId": "63b75648-721d-4bc1-e17d-4fbf6d92a66c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# word2vec_model = gensim.downloader.load('word2vec-google-news-300')\n",
    "# word2vec_model.save('Gensim_word2vec_model.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70cad8bc",
   "metadata": {
    "id": "70cad8bc"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec_model= KeyedVectors.load(\"Gensim_word2vec_model.kv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c94ec17",
   "metadata": {
    "id": "9c94ec17"
   },
   "source": [
    "### Process to extract word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nneMpPJDOVWX",
   "metadata": {
    "id": "nneMpPJDOVWX"
   },
   "outputs": [],
   "source": [
    "# embedding_space_concat = []\n",
    "# for i in range(60000):\n",
    "#     vectorWord = np.zeros((1,300))  # change the size of the vector\n",
    "#     listword = amazon_df['review_body'][i].split(\" \")\n",
    "#     for item in listword[:20]:\n",
    "#         if item in word2vec_model:\n",
    "#             vectorWord = np.concatenate([vectorWord, np.expand_dims(word2vec_model[item], axis=0)], axis=0)\n",
    "\n",
    "#     vectorWord = vectorWord[1:]\n",
    "#     if len(vectorWord)<20:\n",
    "#         for i in range(20 - len(vectorWord)):\n",
    "#             vectorWord = np.concatenate([vectorWord, np.zeros((1,300))], axis=0)         \n",
    "#     embedding_space_concat.append(vectorWord)\n",
    "    \n",
    "# embedding_dataset_concat = np.array(embedding_space_concat)\n",
    "# embedding_dataset_concat = embedding_dataset_concat.reshape(embedding_dataset_concat.shape[0], embedding_dataset_concat.shape[2])\n",
    "# embedding_dataset_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa9e771f",
   "metadata": {
    "id": "aa9e771f"
   },
   "outputs": [],
   "source": [
    "embedding_space_concat = []\n",
    "for i in range(60000):\n",
    "    vectorWord = [] # change the size of the vector\n",
    "    listword = amazon_df['review_body'][i].split(\" \")\n",
    "    for item in listword[:20]:\n",
    "        if item in word2vec_model:\n",
    "            x=np.reshape(word2vec_model[item], (1, 300))\n",
    "            vectorWord.append(x)\n",
    "    vectorWord=vectorWord[1:]\n",
    "    if len(vectorWord) < 20:\n",
    "        di = 20 - len(vectorWord)\n",
    "        vectorWord += [np.zeros((1, 300))] * di\n",
    "            \n",
    "    embedding_space_concat.append(vectorWord)\n",
    "embedding_dataset_concat=np.array(embedding_space_concat)\n",
    "embedding_dataset_concat=embedding_dataset_concat.reshape(embedding_dataset_concat.shape[0], embedding_dataset_concat.shape[1], embedding_dataset_concat.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "pHO9ryGTX0kt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pHO9ryGTX0kt",
    "outputId": "cc2d2a67-ded9-48d6-c0b8-0fe5c5dc2e74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 20, 300)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dataset_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "LBPkdh5igFMc",
   "metadata": {
    "id": "LBPkdh5igFMc"
   },
   "outputs": [],
   "source": [
    "A_train, A_test, B_train, B_test = train_test_split(embedding_dataset_concat, amazon_df['class'], test_size=0.20, random_state=42, stratify=amazon_df['class'])\n",
    "\n",
    "B_train = B_train.reset_index(drop=True)\n",
    "B_test = B_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1825c7c4",
   "metadata": {
    "id": "1825c7c4"
   },
   "source": [
    "## 5. Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6069036d",
   "metadata": {
    "id": "6069036d"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32ee0a2c",
   "metadata": {
    "id": "32ee0a2c"
   },
   "outputs": [],
   "source": [
    "#Creating a dataloader using torch\n",
    "class dataloader(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_record, label_record):\n",
    "        self.dataset = dataset_record\n",
    "        self.labels = label_record\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        dataset = self.dataset[index]\n",
    "        labels  = self.labels[index]\n",
    "        \n",
    "        return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "zmPEPnpHUouD",
   "metadata": {
    "id": "zmPEPnpHUouD"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, classes, layer, batch_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(300, 600, layer, batch_first=True)\n",
    "        self.h1 = torch.randn(layer, batch_size, 600)\n",
    "        self.linear = nn.Linear(600, classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(self.rnn(x)[0][:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "VcLYplBSU8l0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VcLYplBSU8l0",
    "outputId": "888a6641-afea-4c48-df0e-378b95f2f8f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (rnn): RNN(300, 600, num_layers=2, batch_first=True)\n",
       "  (linear): Linear(in_features=600, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_m = RNN(3, 2, 100)\n",
    "rnn_m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a845cb38",
   "metadata": {
    "id": "a845cb38"
   },
   "source": [
    "### 5. (a) Non Gated RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7171096",
   "metadata": {
    "id": "d7171096"
   },
   "outputs": [],
   "source": [
    "# Convert A_train and A_test to float32 \n",
    "A_word2vec_train = A_train.astype(np.float32)\n",
    "A_word2vec_test  = A_test.astype(np.float32)\n",
    "\n",
    "# Subtract 1 from B_train and B_test values\n",
    "B_train = B_train - 1\n",
    "B_test = B_test - 1\n",
    "\n",
    "# Create PyTorch DataLoader objects for the training and testing sets\n",
    "train_dataset = dataloader(A_word2vec_train, B_train)\n",
    "train_set = torch.utils.data.DataLoader(train_dataset, batch_size=100)\n",
    "\n",
    "test_dataset = dataloader(A_word2vec_test, B_test)\n",
    "test_set = torch.utils.data.DataLoader(test_dataset, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "smcVtvTOyavb",
   "metadata": {
    "id": "smcVtvTOyavb"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b31d304",
   "metadata": {
    "id": "4b31d304"
   },
   "outputs": [],
   "source": [
    "def train(reviews_dataloader_train, reviews_dataloader_test, model, num_epochs, concat=True, rnn=True, gru=False, prev_loss=float('inf')):\n",
    "    y_pred_label_train = []\n",
    "    y_true_label_train = []\n",
    "    y_pred_label_test = []\n",
    "    y_true_label_test = []\n",
    "    \n",
    "    # Set the device for the model\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # model.to(device)\n",
    "    \n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=0.001)\n",
    "    # optimizer = SGD(rnn.parameters(), lr=1e-2)\n",
    "    scheduler = ReduceLROnPlateau(optimizer)\n",
    "    \n",
    "    # optimizer = Adam(model.parameters(), lr=0.001)\n",
    "    softmax = Softmax(dim=1)\n",
    "    \n",
    "    # Define the scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "    \n",
    "    # Keep track of the best model\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # Keep track of the previous loss\n",
    "    loss_min = prev_loss\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        print('\\n Epoch: {}'.format(epoch))\n",
    "        \n",
    "        # print(reviews_dataloader_train)\n",
    "        for j, (x, y) in enumerate(reviews_dataloader_train):\n",
    "            y_pred = model(x)\n",
    "            y_pred_label_train.append(torch.argmax(softmax(y_pred.detach()), axis=1))\n",
    "            y_true_label_train.append(y.detach())\n",
    "            loss = criterion(y_pred, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # if j % 100 == 0:\n",
    "            #     print('Epoch {:03} Batch {:03}/{:03} Loss: {:.4f}'.format(epoch, j, len(reviews_dataloader_train), loss.item()))\n",
    "                \n",
    "        # Evaluate the model on the test set\n",
    "        with torch.no_grad():\n",
    "            for x, y in reviews_dataloader_test:\n",
    "                y_pred = model(x)\n",
    "                y_pred_label_test.append(torch.argmax(softmax(y_pred.detach()), axis=1))\n",
    "                y_true_label_test.append(y.detach())\n",
    "\n",
    "        # Calculate accuracy and f1-score\n",
    "        y_pred_train = torch.cat(y_pred_label_train)\n",
    "        y_true_train = torch.cat(y_true_label_train)\n",
    "        y_pred_test = torch.cat(y_pred_label_test)\n",
    "        y_true_test = torch.cat(y_true_label_test)\n",
    "        \n",
    "        train_acc = accuracy_score(y_true_train.cpu().numpy(), y_pred_train.cpu().numpy())\n",
    "        test_acc = accuracy_score(y_true_test.cpu().numpy(), y_pred_test.cpu().numpy())\n",
    "        train_f1 = f1_score(y_true_train.cpu().numpy(), y_pred_train.cpu().numpy(), average='macro')\n",
    "        test_f1 = f1_score(y_true_test.cpu().numpy(), y_pred_test.cpu().numpy(), average='macro')\n",
    "\n",
    "        print('Epoch: {:03}, Loss: {:.4f}, Train Acc: {:.4f}, Test Acc: {:.4f}'.format(epoch, loss.item(), train_acc, test_acc))\n",
    "        \n",
    "        # Update the learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save the best model based on test accuracy\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "        # Save the model checkpoint\n",
    "        # if loss.item() < loss_min:\n",
    "        #     print(f'Loss decreased from {loss_min:.4f} to {loss.item():.4f}. Saving model...')\n",
    "        #     torch.save(model.state_dict(), 'model_checkpoint.pt')\n",
    "        #     loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84c5dd8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84c5dd8f",
    "outputId": "368ab08f-9ca3-4ddd-fc20-5d8a6882e59e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch: 0\n",
      "Epoch: 000, Loss: 1.0730, Train Acc: 0.3482, Test Acc: 0.3767\n",
      "\n",
      " Epoch: 1\n",
      "Epoch: 001, Loss: 1.1250, Train Acc: 0.3590, Test Acc: 0.3447\n",
      "\n",
      " Epoch: 2\n",
      "Epoch: 002, Loss: 1.1038, Train Acc: 0.3572, Test Acc: 0.3496\n",
      "\n",
      " Epoch: 3\n",
      "Epoch: 003, Loss: 1.1070, Train Acc: 0.3566, Test Acc: 0.3521\n",
      "\n",
      " Epoch: 4\n",
      "Epoch: 004, Loss: 1.1113, Train Acc: 0.3562, Test Acc: 0.3537\n",
      "\n",
      " Epoch: 5\n",
      "Epoch: 005, Loss: 1.0876, Train Acc: 0.3581, Test Acc: 0.3576\n",
      "\n",
      " Epoch: 6\n",
      "Epoch: 006, Loss: 1.0864, Train Acc: 0.3598, Test Acc: 0.3606\n",
      "\n",
      " Epoch: 7\n",
      "Epoch: 007, Loss: 1.0828, Train Acc: 0.3612, Test Acc: 0.3629\n",
      "\n",
      " Epoch: 8\n",
      "Epoch: 008, Loss: 1.0826, Train Acc: 0.3623, Test Acc: 0.3647\n",
      "\n",
      " Epoch: 9\n",
      "Epoch: 009, Loss: 1.0824, Train Acc: 0.3632, Test Acc: 0.3661\n",
      "\n",
      " Epoch: 10\n",
      "Epoch: 010, Loss: 1.0850, Train Acc: 0.3645, Test Acc: 0.3672\n",
      "\n",
      " Epoch: 11\n",
      "Epoch: 011, Loss: 1.0849, Train Acc: 0.3657, Test Acc: 0.3682\n",
      "\n",
      " Epoch: 12\n",
      "Epoch: 012, Loss: 1.0849, Train Acc: 0.3666, Test Acc: 0.3690\n",
      "\n",
      " Epoch: 13\n",
      "Epoch: 013, Loss: 1.0849, Train Acc: 0.3674, Test Acc: 0.3697\n",
      "\n",
      " Epoch: 14\n",
      "Epoch: 014, Loss: 1.0849, Train Acc: 0.3682, Test Acc: 0.3703\n",
      "\n",
      " Epoch: 15\n",
      "Epoch: 015, Loss: 1.0849, Train Acc: 0.3689, Test Acc: 0.3709\n",
      "\n",
      " Epoch: 16\n",
      "Epoch: 016, Loss: 1.0850, Train Acc: 0.3695, Test Acc: 0.3713\n",
      "\n",
      " Epoch: 17\n",
      "Epoch: 017, Loss: 1.0850, Train Acc: 0.3701, Test Acc: 0.3717\n",
      "\n",
      " Epoch: 18\n",
      "Epoch: 018, Loss: 1.0850, Train Acc: 0.3706, Test Acc: 0.3721\n",
      "\n",
      " Epoch: 19\n",
      "Epoch: 019, Loss: 1.0850, Train Acc: 0.3710, Test Acc: 0.3725\n"
     ]
    }
   ],
   "source": [
    "train(train_set, test_set, rnn_m, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EzciBwWD4fbc",
   "metadata": {
    "id": "EzciBwWD4fbc"
   },
   "source": [
    "## 5. (b) Gated RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "oTGjstW1atVw",
   "metadata": {
    "id": "oTGjstW1atVw"
   },
   "outputs": [],
   "source": [
    "class GatedRNN(nn.Module):\n",
    "    def __init__(self, num_classes, layers, batch_size):\n",
    "        super(GatedRNN, self).__init__()\n",
    "        self.rnn = nn.GRU(300, 300, layers, batch_first=True)\n",
    "        self.h1 = torch.randn(layers, batch_size, 300)\n",
    "        self.linear = nn.Linear(300, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(self.rnn(x)[0][:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "UpR1pR1Fa0vt",
   "metadata": {
    "id": "UpR1pR1Fa0vt"
   },
   "outputs": [],
   "source": [
    "gru = GatedRNN(5, 1, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "gSD_AfRS3gaG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gSD_AfRS3gaG",
    "outputId": "7d70bbe6-4d39-466e-e007-509e9fc9ca8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch: 0\n",
      "Epoch: 000, Loss: 0.9270, Train Acc: 0.5037, Test Acc: 0.5558\n",
      "\n",
      " Epoch: 1\n",
      "Epoch: 001, Loss: 0.8748, Train Acc: 0.5415, Test Acc: 0.5694\n",
      "\n",
      " Epoch: 2\n",
      "Epoch: 002, Loss: 0.8604, Train Acc: 0.5606, Test Acc: 0.5769\n",
      "\n",
      " Epoch: 3\n",
      "Epoch: 003, Loss: 0.8465, Train Acc: 0.5739, Test Acc: 0.5822\n",
      "\n",
      " Epoch: 4\n",
      "Epoch: 004, Loss: 0.8341, Train Acc: 0.5850, Test Acc: 0.5863\n",
      "\n",
      " Epoch: 5\n",
      "Epoch: 005, Loss: 0.8319, Train Acc: 0.5962, Test Acc: 0.5909\n",
      "\n",
      " Epoch: 6\n",
      "Epoch: 006, Loss: 0.8230, Train Acc: 0.6048, Test Acc: 0.5939\n",
      "\n",
      " Epoch: 7\n",
      "Epoch: 007, Loss: 0.8148, Train Acc: 0.6116, Test Acc: 0.5961\n",
      "\n",
      " Epoch: 8\n",
      "Epoch: 008, Loss: 0.8064, Train Acc: 0.6173, Test Acc: 0.5978\n",
      "\n",
      " Epoch: 9\n",
      "Epoch: 009, Loss: 0.7969, Train Acc: 0.6221, Test Acc: 0.5992\n",
      "\n",
      " Epoch: 10\n",
      "Epoch: 010, Loss: 0.7995, Train Acc: 0.6266, Test Acc: 0.6007\n",
      "\n",
      " Epoch: 11\n",
      "Epoch: 011, Loss: 0.7995, Train Acc: 0.6304, Test Acc: 0.6019\n",
      "\n",
      " Epoch: 12\n",
      "Epoch: 012, Loss: 0.7987, Train Acc: 0.6336, Test Acc: 0.6029\n",
      "\n",
      " Epoch: 13\n",
      "Epoch: 013, Loss: 0.7977, Train Acc: 0.6364, Test Acc: 0.6037\n",
      "\n",
      " Epoch: 14\n",
      "Epoch: 014, Loss: 0.7966, Train Acc: 0.6389, Test Acc: 0.6044\n",
      "\n",
      " Epoch: 15\n",
      "Epoch: 015, Loss: 0.7967, Train Acc: 0.6411, Test Acc: 0.6050\n",
      "\n",
      " Epoch: 16\n",
      "Epoch: 016, Loss: 0.7967, Train Acc: 0.6430, Test Acc: 0.6056\n",
      "\n",
      " Epoch: 17\n",
      "Epoch: 017, Loss: 0.7967, Train Acc: 0.6447, Test Acc: 0.6061\n",
      "\n",
      " Epoch: 18\n",
      "Epoch: 018, Loss: 0.7967, Train Acc: 0.6463, Test Acc: 0.6065\n",
      "\n",
      " Epoch: 19\n",
      "Epoch: 019, Loss: 0.7967, Train Acc: 0.6476, Test Acc: 0.6069\n"
     ]
    }
   ],
   "source": [
    "train(train_set, test_set, gru, 20,True,True, True)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
