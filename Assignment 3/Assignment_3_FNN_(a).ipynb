{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b408ad95",
   "metadata": {
    "id": "b408ad95"
   },
   "source": [
    "## Installed Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31912917",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31912917",
    "outputId": "c9617a8e-2041-44b1-db68-1fb7b8294767",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting anyascii\n",
      "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyahocorasick\n",
      "  Downloading pyahocorasick-2.0.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (104 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.5/104.5 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.1 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting gensim==4.2.0\n",
      "  Downloading gensim-4.2.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim==4.2.0) (6.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.8/dist-packages (from gensim==4.2.0) (1.22.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim==4.2.0) (1.10.1)\n",
      "Installing collected packages: gensim\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 3.6.0\n",
      "    Uninstalling gensim-3.6.0:\n",
      "      Successfully uninstalled gensim-3.6.0\n",
      "Successfully installed gensim-4.2.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.2.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.22.4)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.1+cu116)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.8/dist-packages (0.13.1+cu116)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.22.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.25.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.26.14)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install contractions\n",
    "!{sys.executable} -m pip install gensim==4.2.0\n",
    "!pip install scikit-learn\n",
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cac7ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9cac7ca",
    "outputId": "8ad87edb-b96f-4bc4-cde0-cfdbea041e1b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "## Importing and installing libraries\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import warnings \n",
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from torch import nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss, Softmax, Linear\n",
    "from torch.optim import SGD, Adam\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import utils\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from statistics import mean\n",
    "from os import path\n",
    "import os.path\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50330ee0",
   "metadata": {
    "id": "50330ee0"
   },
   "source": [
    "## 1. Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XthACUqG0PBl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XthACUqG0PBl",
    "outputId": "dc5867fe-d618-4d5a-d86f-21cab66c8b94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n",
      "/content/drive/My Drive/Colab Notebooks\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "%cd /content/drive/My Drive/Colab Notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791a4b96",
   "metadata": {
    "id": "791a4b96"
   },
   "outputs": [],
   "source": [
    "#fields required in the balanced dataframe from the original dataset\n",
    "input_column=[\"review_body\",\"star_rating\"]\n",
    "\n",
    "#reading the original dataset to filter the columns that are required\n",
    "input_df =pd.read_csv('./amazon_reviews_us_Beauty_v1_00.tsv',usecols=input_column,sep='\\t',error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3317dfe3",
   "metadata": {
    "id": "3317dfe3"
   },
   "outputs": [],
   "source": [
    "#Creating 3 different classes to get 20000 data from each class to avoid computational burden\n",
    "\n",
    "class_one_df =(input_df[(input_df['star_rating'] == 1) | (input_df['star_rating'] == 2) ]).sample(n=20000)\n",
    "class_one_df['class']=1\n",
    "\n",
    "class_two_df =(input_df[(input_df['star_rating'] == 3)]).sample(n=20000)\n",
    "class_two_df['class']=2\n",
    "\n",
    "class_three_df =(input_df[(input_df['star_rating'] == 4) | (input_df['star_rating'] == 5) ]).sample(n=20000)\n",
    "class_three_df['class']=3\n",
    "\n",
    "#Combining all the data received from each class into a single balanced dataframe\n",
    "\n",
    "amazon_balanced_df = pd.concat([class_one_df, class_two_df, class_three_df])\n",
    "\n",
    "#Resetting the index as we have retrieved different data according to the classes created.\n",
    "#Therefore, we will have irregular or unsorted index keys. \n",
    "#We will reset the index to the new and incremental values from 0\n",
    "\n",
    "amazon_balanced_df = amazon_balanced_df.reset_index(drop=True)\n",
    "\n",
    "# Created a new dataframe consisting of the two columns (star_rating and review_body) \n",
    "#along with class one assigned to them on the basis of star_rating. We are also resetting the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c26f1c1",
   "metadata": {
    "id": "2c26f1c1"
   },
   "source": [
    "### Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3d5ab5",
   "metadata": {
    "id": "2f3d5ab5"
   },
   "source": [
    "### Handling null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b712481",
   "metadata": {
    "id": "3b712481"
   },
   "outputs": [],
   "source": [
    "#We are changing all null values to an empty string\n",
    "\n",
    "amazon_balanced_df = amazon_balanced_df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816443d1",
   "metadata": {
    "id": "816443d1"
   },
   "outputs": [],
   "source": [
    "#Uncleaned data copy\n",
    "amazon_df=amazon_balanced_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de263c8c",
   "metadata": {
    "id": "de263c8c"
   },
   "source": [
    "### Convert all reviews into lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7707d5",
   "metadata": {
    "id": "6e7707d5"
   },
   "outputs": [],
   "source": [
    "# Converting all review body into lowercase\n",
    "\n",
    "amazon_balanced_df['review_body'] = amazon_balanced_df['review_body'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148e2913",
   "metadata": {
    "id": "148e2913"
   },
   "source": [
    "### Remove the HTML from the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ca361f",
   "metadata": {
    "id": "32ca361f"
   },
   "outputs": [],
   "source": [
    "# Removing all the html tags from each review body \n",
    "\n",
    "amazon_balanced_df['review_body']=amazon_balanced_df['review_body'].apply(lambda x : re.sub('<.*?>','',str(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e00124",
   "metadata": {
    "id": "c7e00124"
   },
   "source": [
    "### Remove the URLs from the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6904642",
   "metadata": {
    "id": "e6904642"
   },
   "outputs": [],
   "source": [
    "# Removing all the URLs from each review body \n",
    "\n",
    "amazon_balanced_df['review_body'] = amazon_balanced_df['review_body'].apply(lambda y: re.split('https:\\/\\/.*', str(y))[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec91edcf",
   "metadata": {
    "id": "ec91edcf"
   },
   "source": [
    "### Remove non-alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43757d50",
   "metadata": {
    "id": "43757d50"
   },
   "outputs": [],
   "source": [
    "# Removing all the non alphabetic chaarcters(symbols, numbers) from each review body \n",
    "\n",
    "amazon_balanced_df['review_body'] = amazon_balanced_df['review_body'].apply(lambda z: \" \".join([re.sub('[^A-Za-z]+','', z) for z in nltk.word_tokenize(z)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5e594d",
   "metadata": {
    "id": "3e5e594d"
   },
   "source": [
    "### Remove extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d078c690",
   "metadata": {
    "id": "d078c690"
   },
   "outputs": [],
   "source": [
    "# Will remove leading and trailing spaces\n",
    "amazon_balanced_df['review_body'] = amazon_balanced_df['review_body'].str.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7769377",
   "metadata": {
    "id": "d7769377"
   },
   "source": [
    "### Perform contractions on the review_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f866c40",
   "metadata": {
    "id": "0f866c40"
   },
   "outputs": [],
   "source": [
    "## This will elongate the short form used in sentences like (I'll ---> I will)\n",
    "\n",
    "amazon_balanced_df['without_contraction'] = amazon_balanced_df['review_body'].apply(lambda a: [contractions.fix(word) for word in a.split()])\n",
    "amazon_balanced_df['review_body'] = [' '.join(map(str, x)) for x in amazon_balanced_df['without_contraction']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d8987",
   "metadata": {
    "id": "eb8d8987"
   },
   "source": [
    "## Remove Punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a428a2",
   "metadata": {
    "id": "f7a428a2"
   },
   "outputs": [],
   "source": [
    "amazon_balanced_df['review_body'] = amazon_balanced_df['review_body'].str.replace(r'[^\\w\\s]+', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6866fdc8",
   "metadata": {
    "id": "6866fdc8"
   },
   "source": [
    "## 2. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4d2c8d",
   "metadata": {
    "id": "0f4d2c8d"
   },
   "source": [
    "### (a) Downloading pretrained word2vec-google-news-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cad8bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70cad8bc",
    "outputId": "710d671d-af5f-4c93-ed82-05ee2d4bfda9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================-] 99.7% 1657.5/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "word2vec_model = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i_-bevYMylYS",
   "metadata": {
    "id": "i_-bevYMylYS"
   },
   "outputs": [],
   "source": [
    "word2vec_model.save('gensim2.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53b477b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c53b477b",
    "outputId": "1bcabe57-85d6-4312-fa8e-61e9208da411"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7300518]]\n",
      "[[0.6510957]]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity([word2vec_model['queen']], [word2vec_model['king'] - word2vec_model['man'] + word2vec_model['woman']]))\n",
    "print(cosine_similarity([word2vec_model['queen']], [word2vec_model['king']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a67ed67",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4a67ed67",
    "outputId": "8617aaf2-1291-456d-a5ee-55493da03e56",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 0.7291510105133057),\n",
       " ('bad', 0.7190051078796387),\n",
       " ('terrific', 0.6889115571975708),\n",
       " ('decent', 0.6837348341941833),\n",
       " ('nice', 0.6836092472076416),\n",
       " ('excellent', 0.644292950630188),\n",
       " ('fantastic', 0.6407778263092041),\n",
       " ('better', 0.6120728850364685),\n",
       " ('solid', 0.5806034803390503),\n",
       " ('lousy', 0.576420247554779)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.most_similar(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6745ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fb6745ee",
    "outputId": "93344929-c31b-4085-bf00-4ba9bf5d9834"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7814771"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.similarity(w1=\"daughter\", w2=\"sister\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af3c50",
   "metadata": {
    "id": "d0af3c50"
   },
   "source": [
    "### (b) Training word2vec model on our own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dafe8b",
   "metadata": {
    "id": "61dafe8b"
   },
   "outputs": [],
   "source": [
    "class dataEmbed:\n",
    "    def __init__(self, data_set):\n",
    "      self.data_set = data_set\n",
    "\n",
    "    def __iter__(self):\n",
    "        for x in self.data_set:\n",
    "            yield utils.simple_preprocess(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caedc654",
   "metadata": {
    "id": "caedc654"
   },
   "outputs": [],
   "source": [
    "sentence_embed = dataEmbed(amazon_balanced_df.review_body)\n",
    "# window=13\n",
    "# vector_size=300\n",
    "# min_count=9\n",
    "embed_word2vec = Word2Vec(sentences=sentence_embed, vector_size=300, min_count=9, window=13)\n",
    "model = embed_word2vec.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81275c2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81275c2d",
    "outputId": "3a0d705a-355a-4553-9a74-eefd594e75a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1676019]]\n",
      "[[0.40402395]]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity([model['queen']], [model['king'] - model['man'] + model['woman']]))\n",
    "print(cosine_similarity([model['queen']], [model['king']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6245c6e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c6245c6e",
    "outputId": "d1a7a804-3f46-49b9-fb36-c387175b1faa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 0.7492009997367859),\n",
       " ('decent', 0.6979432106018066),\n",
       " ('nice', 0.6569323539733887),\n",
       " ('fantastic', 0.600950300693512),\n",
       " ('ok', 0.5753679275512695),\n",
       " ('bad', 0.5533730387687683),\n",
       " ('okay', 0.5282017588615417),\n",
       " ('alright', 0.5122868418693542),\n",
       " ('awesome', 0.5109883546829224),\n",
       " ('high', 0.4818119406700134)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20d5a69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f20d5a69",
    "outputId": "f252fe5f-2d03-4430-8d99-ff60975b6821"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8866891"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(w1=\"daughter\", w2=\"sister\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098b5c1c",
   "metadata": {
    "id": "098b5c1c"
   },
   "source": [
    "## 3. Simple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f5c771",
   "metadata": {
    "id": "62f5c771"
   },
   "source": [
    "### Split the data into 80% train and 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5fa9e8",
   "metadata": {
    "id": "1d5fa9e8"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(amazon_balanced_df['review_body'], amazon_balanced_df['class'], test_size=0.20, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e703dea6",
   "metadata": {
    "id": "e703dea6"
   },
   "outputs": [],
   "source": [
    "# print(\"Train Size \", X_train.shape)\n",
    "# print(\"Test Size \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e3c0f3",
   "metadata": {
    "id": "22e3c0f3"
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2933ecfc",
   "metadata": {
    "id": "2933ecfc"
   },
   "outputs": [],
   "source": [
    "Tf_Id_Vector = TfidfVectorizer(\n",
    "ngram_range=(1, 2),\n",
    "analyzer='word',\n",
    "token_pattern=r'\\w{1,}',\n",
    "strip_accents='unicode',\n",
    "max_features=10000,\n",
    "stop_words='english'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937cf233",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "937cf233",
    "outputId": "ee9f0530-b750-4e3e-ce60-20ee6de637a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train feature names ['aa' 'ability' 'able' ... 'zipper' 'zits' 'zone']\n"
     ]
    }
   ],
   "source": [
    "X_tf_id_train = Tf_Id_Vector.fit_transform(X_train)\n",
    "X_tf_id_test = Tf_Id_Vector.transform(X_test)\n",
    "print(\"Train feature names\", Tf_Id_Vector.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c405fee3",
   "metadata": {
    "id": "c405fee3"
   },
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7286ed03",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7286ed03",
    "outputId": "ead3c359-023e-4d96-d751-a6b7d72379f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Values for Perceptron Model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.69      0.65      4029\n",
      "           2       0.52      0.51      0.52      3990\n",
      "           3       0.73      0.67      0.70      3981\n",
      "\n",
      "    accuracy                           0.62     12000\n",
      "   macro avg       0.62      0.62      0.62     12000\n",
      "weighted avg       0.62      0.62      0.62     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron(\n",
    "penalty= 'l1', #Provided to model on wrong prediction\n",
    "alpha=0.000005, #Strength of the L2 regularization term\n",
    "max_iter=1500, #Maximum number of iterations over the data\n",
    "tol=1e-1, #Tolerance for the optimization or the criterion for stopping\n",
    ")\n",
    "perceptron.fit(X_tf_id_train , y_train)\n",
    "#prediction through X_test data\n",
    "\n",
    "y_test_prediction=perceptron.predict(X_tf_id_test)\n",
    "#Preparing the report by comparing actual value and predicted value\n",
    "report=classification_report(y_test, y_test_prediction)\n",
    "print(\"\\n Values for Perceptron Model\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b345b37",
   "metadata": {
    "id": "2b345b37"
   },
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a7de61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2a7de61",
    "outputId": "caffb631-2df1-4dae-9f39-829003a3fd57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Values for SVM Model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.69      0.70      0.70      4029\n",
      "           2       0.61      0.56      0.58      3990\n",
      "           3       0.74      0.79      0.76      3981\n",
      "\n",
      "    accuracy                           0.68     12000\n",
      "   macro avg       0.68      0.68      0.68     12000\n",
      "weighted avg       0.68      0.68      0.68     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(\n",
    "C=0.10, #Regularization parameter. Default is 1.\n",
    "penalty='l2', #Norm of Penalty\n",
    "tol=1e-1, #tolerance of Stopping criteria. default is 1e-3\n",
    "class_weight=\"balanced\", #adjust and provides weight to each class\n",
    "max_iter=1000, #Hard limit on iterations within solver, or -1 for no limit.\n",
    "random_state=1, #Controls the pseudo random number generation for shuffling the data for probability estimates.\n",
    "loss='squared_hinge', #Specifies the Loss Function\n",
    "dual=False, #Selects the algorithm to either the dual or primal optimization\n",
    "fit_intercept=False, #Weather to calculate intercept for this model\n",
    ")\n",
    "svm.fit(X_tf_id_train, y_train)\n",
    "#prediction through X_test data\n",
    "y_test_prediction_svm=svm.predict(X_tf_id_test)\n",
    "#Preparing the report by comparing actual value and predicted value\n",
    "report_svm=classification_report(y_test, y_test_prediction_svm)\n",
    "print(\"\\n Values for SVM Model\")\n",
    "print(report_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c94ec17",
   "metadata": {
    "id": "9c94ec17"
   },
   "source": [
    "### Process to extract word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e771f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa9e771f",
    "outputId": "33bc1426-843e-4c35-df31-92e7d51e11e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 300)\n"
     ]
    }
   ],
   "source": [
    "embedding_space = []\n",
    "for i in range(60000):\n",
    "    vectorWord = np.zeros((1,300))\n",
    "    listword = amazon_df['review_body'][i].split(\" \")\n",
    "    for word in listword:\n",
    "        if word in word2vec_model.key_to_index:\n",
    "            np.reshape(word2vec_model[word], (1, 300))\n",
    "            vectorWord += word2vec_model[word]\n",
    "        else:\n",
    "            vectorWord += np.zeros((1,300))           \n",
    "    avg_wordVec = vectorWord/len(listword)\n",
    "    embedding_space.append(avg_wordVec)\n",
    "    \n",
    "    \n",
    "embedding_dataset = np.array(embedding_space)\n",
    "print(embedding_dataset.shape)\n",
    "embedding_dataset = embedding_dataset.reshape(embedding_dataset.shape[0], embedding_dataset.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z4tk-ZeQWNwE",
   "metadata": {
    "id": "z4tk-ZeQWNwE"
   },
   "outputs": [],
   "source": [
    "embedding_space_concat = []\n",
    "for i in range(60000):\n",
    "    vectorWord = [] # change the size of the vector\n",
    "    listword = amazon_df['review_body'][i].split(\" \")\n",
    "    for item in listword[:20]:\n",
    "        if item in word2vec_model:\n",
    "            x=np.reshape(word2vec_model[item], (1, 300))\n",
    "            vectorWord.append(x)\n",
    "    vectorWord=vectorWord[1:]\n",
    "    if len(vectorWord) < 20:\n",
    "        di = 20 - len(vector_word)\n",
    "        vectorWord += [np.zeros((1, 300))] * di\n",
    "            \n",
    "    embedding_space_concat.append(vectorWord)\n",
    "embedding_space_concat=np.array(embedding_space_concat)\n",
    "embedding_dataset=embedding_space_concat.reshape(embedding_space_concat.shape[0], embedding_space_concat.shape[1], embedding_space_concat.shape[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45634361",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45634361",
    "outputId": "ec12ccbc-c488-4eee-b41d-beb0fb88e3d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 300) (12000, 300) (48000,) (12000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "A_train, A_test, B_train, B_test = train_test_split(embedding_dataset, amazon_df['class'], test_size=0.20, random_state=42, stratify=amazon_df['class'])\n",
    "\n",
    "B_train = B_train.reset_index(drop=True)\n",
    "B_test = B_test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(A_train.shape, A_test.shape, B_train.shape, B_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2558ab69",
   "metadata": {
    "id": "2558ab69"
   },
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646e6556",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "646e6556",
    "outputId": "16aafe5f-3339-4965-b9ad-32ba3acd6141"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Values for Perceptron Model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.50      0.58      4000\n",
      "           2       0.58      0.32      0.41      4000\n",
      "           3       0.51      0.87      0.65      4000\n",
      "\n",
      "    accuracy                           0.56     12000\n",
      "   macro avg       0.59      0.56      0.54     12000\n",
      "weighted avg       0.59      0.56      0.54     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron(\n",
    "penalty= 'l1', #Provided to model on wrong prediction\n",
    "alpha=0.000005, #Strength of the L2 regularization term\n",
    "max_iter=1500, #Maximum number of iterations over the data\n",
    "tol=1e-1, #Tolerance for the optimization or the criterion for stopping\n",
    ")\n",
    "perceptron.fit(A_train , B_train)\n",
    "#prediction through X_test data\n",
    "\n",
    "B_test_prediction=perceptron.predict(A_test)\n",
    "#Preparing the report by comparing actual value and predicted value\n",
    "report=classification_report(B_test, B_test_prediction)\n",
    "print(\"\\n Values for Perceptron Model\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1448a0",
   "metadata": {
    "id": "4b1448a0"
   },
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526cf805",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "526cf805",
    "outputId": "f07e79a7-09bb-4a20-ea86-cb601bcdd5b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Values for SVM Model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.66      0.63      4000\n",
      "           2       0.56      0.49      0.53      4000\n",
      "           3       0.67      0.70      0.68      4000\n",
      "\n",
      "    accuracy                           0.62     12000\n",
      "   macro avg       0.61      0.62      0.61     12000\n",
      "weighted avg       0.61      0.62      0.61     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(\n",
    "C=0.10, #Regularization parameter. Default is 1.\n",
    "penalty='l2', #Norm of Penalty\n",
    "tol=1e-1, #tolerance of Stopping criteria. default is 1e-3\n",
    "class_weight=\"balanced\", #adjust and provides weight to each class\n",
    "max_iter=1000, #Hard limit on iterations within solver, or -1 for no limit.\n",
    "random_state=1, #Controls the pseudo random number generation for shuffling the data for probability estimates.\n",
    "loss='squared_hinge', #Specifies the Loss Function\n",
    "dual=False, #Selects the algorithm to either the dual or primal optimization\n",
    "fit_intercept=False, #Weather to calculate intercept for this model\n",
    ")\n",
    "svm.fit(A_train , B_train)\n",
    "#prediction through X_test data\n",
    "B_test_prediction_svm=svm.predict(A_test)\n",
    "#Preparing the report by comparing actual value and predicted value\n",
    "report_svm=classification_report(B_test, B_test_prediction_svm)\n",
    "print(\"\\n Values for SVM Model\")\n",
    "print(report_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1825c7c4",
   "metadata": {
    "id": "1825c7c4"
   },
   "source": [
    "## 4. Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6069036d",
   "metadata": {
    "id": "6069036d"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ee0a2c",
   "metadata": {
    "id": "32ee0a2c"
   },
   "outputs": [],
   "source": [
    "#Creating a dataloader using torch\n",
    "class dataloader(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_record, label_record):\n",
    "        self.dataset = dataset_record\n",
    "        self.labels = label_record\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        dataset = self.dataset[index]\n",
    "        labels  = self.labels[index]\n",
    "        \n",
    "        return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee89b8ef",
   "metadata": {
    "id": "ee89b8ef"
   },
   "outputs": [],
   "source": [
    "#Creating classes to define the architecure \n",
    "class feedForward(nn.Module):\n",
    "    def __init__(self, output_size, input_size):\n",
    "        super(feedForward, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 100)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(100, 10)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(10, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return  self.layer3(self.relu2(self.layer2(self.relu1(self.layer1(x)))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8a040c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb8a040c",
    "outputId": "29e65380-5da1-4a7d-e1c7-f32dc6f3c08d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feedForward(\n",
       "  (layer1): Linear(in_features=300, out_features=100, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (layer2): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (layer3): Linear(in_features=10, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnn=feedForward(3,300)\n",
    "fnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a845cb38",
   "metadata": {
    "id": "a845cb38"
   },
   "source": [
    "### (a) Testing split of Multi layer perceptron and fetching accuracy of FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7171096",
   "metadata": {
    "id": "d7171096"
   },
   "outputs": [],
   "source": [
    "# Convert A_train and A_test to float32 \n",
    "A_word2vec_train = A_train.astype(np.float32)\n",
    "A_word2vec_test  = A_test.astype(np.float32)\n",
    "\n",
    "# Subtract 1 from B_train and B_test values\n",
    "B_train = B_train - 1\n",
    "B_test = B_test - 1\n",
    "\n",
    "# Create PyTorch DataLoader objects for the training and testing sets\n",
    "train_dataset = dataloader(A_word2vec_train, B_train)\n",
    "train_set = torch.utils.data.DataLoader(train_dataset, batch_size=50)\n",
    "\n",
    "test_dataset = dataloader(A_word2vec_test, B_test)\n",
    "test_set = torch.utils.data.DataLoader(test_dataset, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smcVtvTOyavb",
   "metadata": {
    "id": "smcVtvTOyavb"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b31d304",
   "metadata": {
    "id": "4b31d304"
   },
   "outputs": [],
   "source": [
    "def train(reviews_dataloader_train, reviews_dataloader_test, model, num_epochs, concat=False, rnn=False, gru=False, prev_loss=float('inf')):\n",
    "    y_pred_label_train = []\n",
    "    y_true_label_train = []\n",
    "    y_pred_label_test = []\n",
    "    y_true_label_test = []\n",
    "    \n",
    "    # Set the device for the model\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # model.to(device)\n",
    "    \n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=0.001)\n",
    "    softmax = Softmax(dim=1)\n",
    "    \n",
    "    # Define the scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "    \n",
    "    # Keep track of the best model\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # Keep track of the previous loss\n",
    "    loss_min = prev_loss\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        print('\\n Epoch: {}'.format(epoch))\n",
    "        \n",
    "        # print(reviews_dataloader_train)\n",
    "        for j, (x, y) in enumerate(reviews_dataloader_train):\n",
    "            y_pred = model(x)\n",
    "            y_pred_label_train.append(torch.argmax(softmax(y_pred.detach()), axis=1))\n",
    "            y_true_label_train.append(y.detach())\n",
    "            loss = criterion(y_pred, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # if j % 100 == 0:\n",
    "            #     print('Epoch {:03} Batch {:03}/{:03} Loss: {:.4f}'.format(epoch, j, len(reviews_dataloader_train), loss.item()))\n",
    "                \n",
    "        # Evaluate the model on the test set\n",
    "        with torch.no_grad():\n",
    "            for x, y in reviews_dataloader_test:\n",
    "                y_pred = model(x)\n",
    "                y_pred_label_test.append(torch.argmax(softmax(y_pred.detach()), axis=1))\n",
    "                y_true_label_test.append(y.detach())\n",
    "\n",
    "        # Calculate accuracy and f1-score\n",
    "        y_pred_train = torch.cat(y_pred_label_train)\n",
    "        y_true_train = torch.cat(y_true_label_train)\n",
    "        y_pred_test = torch.cat(y_pred_label_test)\n",
    "        y_true_test = torch.cat(y_true_label_test)\n",
    "        \n",
    "        train_acc = accuracy_score(y_true_train.cpu().numpy(), y_pred_train.cpu().numpy())\n",
    "        test_acc = accuracy_score(y_true_test.cpu().numpy(), y_pred_test.cpu().numpy())\n",
    "        train_f1 = f1_score(y_true_train.cpu().numpy(), y_pred_train.cpu().numpy(), average='macro')\n",
    "        test_f1 = f1_score(y_true_test.cpu().numpy(), y_pred_test.cpu().numpy(), average='macro')\n",
    "\n",
    "        print('Epoch: {:03}, Loss: {:.4f}, Train Acc: {:.4f}, Test Acc: {:.4f}'.format(epoch, loss.item(), train_acc, test_acc))\n",
    "        \n",
    "        # Update the learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save the best model based on test accuracy\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "        # Save the model checkpoint\n",
    "        # if loss.item() < loss_min:\n",
    "        #     print(f'Loss decreased from {loss_min:.4f} to {loss.item():.4f}. Saving model...')\n",
    "        #     torch.save(model.state_dict(), 'model_checkpoint.pt')\n",
    "        #     loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c5dd8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84c5dd8f",
    "outputId": "5c38cd65-7141-4e2b-de38-fa342d9b815e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch: 0\n",
      "Epoch: 000, Loss: 0.8441, Train Acc: 0.5594, Test Acc: 0.6198\n",
      "\n",
      " Epoch: 1\n",
      "Epoch: 001, Loss: 0.8092, Train Acc: 0.5902, Test Acc: 0.6267\n",
      "\n",
      " Epoch: 2\n",
      "Epoch: 002, Loss: 0.7992, Train Acc: 0.6043, Test Acc: 0.6308\n",
      "\n",
      " Epoch: 3\n",
      "Epoch: 003, Loss: 0.7932, Train Acc: 0.6129, Test Acc: 0.6334\n",
      "\n",
      " Epoch: 4\n",
      "Epoch: 004, Loss: 0.7883, Train Acc: 0.6187, Test Acc: 0.6351\n",
      "\n",
      " Epoch: 5\n",
      "Epoch: 005, Loss: 0.7794, Train Acc: 0.6241, Test Acc: 0.6365\n",
      "\n",
      " Epoch: 6\n",
      "Epoch: 006, Loss: 0.7783, Train Acc: 0.6281, Test Acc: 0.6376\n",
      "\n",
      " Epoch: 7\n",
      "Epoch: 007, Loss: 0.7764, Train Acc: 0.6312, Test Acc: 0.6384\n",
      "\n",
      " Epoch: 8\n",
      "Epoch: 008, Loss: 0.7749, Train Acc: 0.6337, Test Acc: 0.6391\n",
      "\n",
      " Epoch: 9\n",
      "Epoch: 009, Loss: 0.7736, Train Acc: 0.6357, Test Acc: 0.6396\n",
      "\n",
      " Epoch: 10\n",
      "Epoch: 010, Loss: 0.7743, Train Acc: 0.6374, Test Acc: 0.6399\n",
      "\n",
      " Epoch: 11\n",
      "Epoch: 011, Loss: 0.7742, Train Acc: 0.6388, Test Acc: 0.6402\n",
      "\n",
      " Epoch: 12\n",
      "Epoch: 012, Loss: 0.7740, Train Acc: 0.6400, Test Acc: 0.6404\n",
      "\n",
      " Epoch: 13\n",
      "Epoch: 013, Loss: 0.7739, Train Acc: 0.6410, Test Acc: 0.6407\n",
      "\n",
      " Epoch: 14\n",
      "Epoch: 014, Loss: 0.7737, Train Acc: 0.6419, Test Acc: 0.6408\n",
      "\n",
      " Epoch: 15\n",
      "Epoch: 015, Loss: 0.7739, Train Acc: 0.6427, Test Acc: 0.6410\n",
      "\n",
      " Epoch: 16\n",
      "Epoch: 016, Loss: 0.7740, Train Acc: 0.6434, Test Acc: 0.6412\n",
      "\n",
      " Epoch: 17\n",
      "Epoch: 017, Loss: 0.7741, Train Acc: 0.6440, Test Acc: 0.6414\n",
      "\n",
      " Epoch: 18\n",
      "Epoch: 018, Loss: 0.7742, Train Acc: 0.6445, Test Acc: 0.6415\n",
      "\n",
      " Epoch: 19\n",
      "Epoch: 019, Loss: 0.7742, Train Acc: 0.6450, Test Acc: 0.6417\n"
     ]
    }
   ],
   "source": [
    "train(train_set, test_set, fnn, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gSD_AfRS3gaG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gSD_AfRS3gaG",
    "outputId": "525cb1e2-e204-459d-a61b-4c7919201519"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 3000)\n"
     ]
    }
   ],
   "source": [
    "### To concatenate first 10 Word2Vec vectors for each review as the input feature\n",
    "embedding_space_concat = []\n",
    "for i in range(60000):\n",
    "    vectorWord = np.zeros((1,300*10))  # change the size of the vector\n",
    "    listword = amazon_df['review_body'][i].split(\" \")\n",
    "    for j, word in enumerate(listword):\n",
    "        if j < 10:  # only consider the first 10 words\n",
    "            if word in word2vec_model.key_to_index:\n",
    "                np.reshape(word2vec_model[word], (1, 300))\n",
    "                vectorWord[0, j*300:(j+1)*300] = word2vec_model[word]  # concatenate the vector\n",
    "            else:\n",
    "                vectorWord[0, j*300:(j+1)*300] = np.zeros((1,300))           \n",
    "    embedding_space_concat.append(vectorWord)\n",
    "    \n",
    "embedding_dataset_concat = np.array(embedding_space_concat)\n",
    "print(embedding_dataset_concat.shape)\n",
    "embedding_dataset_concat = embedding_dataset_concat.reshape(embedding_dataset_concat.shape[0], embedding_dataset_concat.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Lc3c_qse1Blr",
   "metadata": {
    "id": "Lc3c_qse1Blr"
   },
   "outputs": [],
   "source": [
    "P_train, P_test, Q_train, Q_test = train_test_split(embedding_space_concat, amazon_df['class'], test_size=0.20, random_state=42, stratify=amazon_df['class'])\n",
    "\n",
    "Q_train = Q_train.reset_index(drop=True)\n",
    "Q_test = Q_test.reset_index(drop=True)\n",
    "\n",
    "P_train= np.array(P_train)\n",
    "P_test= np.array(P_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iECTtqfZ1uNc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iECTtqfZ1uNc",
    "outputId": "5e69d499-18b3-4649-878d-63650b67f721"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(P_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IVTM1kNo1Oj2",
   "metadata": {
    "id": "IVTM1kNo1Oj2"
   },
   "outputs": [],
   "source": [
    "# Convert A_train and A_test to float32 \n",
    "P_word2vec_train = P_train.astype(np.float32)\n",
    "P_word2vec_test  = P_test.astype(np.float32)\n",
    "\n",
    "# Subtract 1 from B_train and B_test values\n",
    "Q_train = Q_train - 1\n",
    "Q_test = Q_test - 1\n",
    "\n",
    "# Create PyTorch DataLoader objects for the training and testing sets\n",
    "train_datasetC = dataloader(P_word2vec_train, Q_train)\n",
    "train_setC = torch.utils.data.DataLoader(train_datasetC, batch_size=50)\n",
    "\n",
    "test_datasetC = dataloader(P_word2vec_test, Q_test)\n",
    "test_setC = torch.utils.data.DataLoader(test_datasetC, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QS_oGh6v1EoD",
   "metadata": {
    "id": "QS_oGh6v1EoD"
   },
   "outputs": [],
   "source": [
    "fnnc=feedForward(3,3000)\n",
    "fnnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kktcC2ca1M7W",
   "metadata": {
    "id": "kktcC2ca1M7W"
   },
   "outputs": [],
   "source": [
    "train(train_setC, test_setC, fnnc, 20, concat=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
